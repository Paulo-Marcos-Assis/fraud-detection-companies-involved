import os
import json
import csv
import time
import signal
from datetime import datetime
from typing import Dict, List
from pathlib import Path
from langchain_core.messages import HumanMessage
from langchain_ollama import ChatOllama

OLLAMA_HOST = os.getenv("OLLAMA_HOST", "https://ollama-dev.ceos.ufsc.br")
SELECTED_MODEL = os.getenv("OLLAMA_MODEL", "gpt-oss:20b")
LLM_TEMPERATURE = 0
TIMEOUT_SECONDS = 180  # Timeout de 180 segundos por not√≠cia
START_FROM = 108518  # √öltima processada: 108517 (12/fev/2026 18:42) - Retomar daqui
MAX_CONSECUTIVE_403_ERRORS = 5  # Parar ap√≥s 5 erros 403 consecutivos

class TimeoutError(Exception):
    """Exce√ß√£o lan√ßada quando o processamento excede o timeout"""
    pass

class OllamaError403(Exception):
    """Exce√ß√£o lan√ßada quando Ollama retorna erro 403"""
    pass

def timeout_handler(signum, frame):
    """Handler para timeout"""
    raise TimeoutError("Processamento excedeu o tempo limite")

class FraudDetector:
    def __init__(self):
        print(f"Fraud Detector configurado para usar Ollama em {OLLAMA_HOST} (modelo: {SELECTED_MODEL})")
        self.llm = None
        self._llm_initialized = False
    
    def _ensure_llm(self):
        if not self._llm_initialized:
            print(f"Conectando ao Ollama em {OLLAMA_HOST}...")
            try:
                self.llm = ChatOllama(
                    model=SELECTED_MODEL,
                    base_url=OLLAMA_HOST,
                    temperature=LLM_TEMPERATURE,
                    timeout=120
                )
                self._llm_initialized = True
                print("Conex√£o com Ollama estabelecida com sucesso!")
            except Exception as e:
                print(f"ERRO ao conectar ao Ollama: {e}")
                self.llm = None
                self._llm_initialized = True

    def analyze_fraud(self, text: str, title: str, timeout: int = TIMEOUT_SECONDS) -> Dict:
        """
        Analisa se a not√≠cia trata de fraudes envolvendo empresas.
        
        Retorna:
        {
            "is_fraud_related": bool,
            "confidence": str,  # "alta", "m√©dia", "baixa"
            "fraud_types": List[str],
            "companies_involved": List[str],
            "summary": str
        }
        """
        default_return = {
            "is_fraud_related": False,
            "confidence": "baixa",
            "fraud_types": [],
            "companies_involved": [],
            "people_involved": [],
            "execution_time_seconds": 0.0
        }
        
        self._ensure_llm()
        
        if not self.llm:
            print("Erro: LLM n√£o inicializado.")
            return default_return
        
        if not text or not isinstance(text, str):
            return default_return

        full_text = f"{title}\n\n{text}" if title else text
        
        prompt_content = f"""
Voc√™ √© um especialista em an√°lise de not√≠cias sobre fraudes empresariais e crimes contra a administra√ß√£o p√∫blica.

Sua tarefa √© analisar a not√≠cia fornecida e determinar se ela trata de fraudes envolvendo empresas.

----------------------------------------------------------------------
INSTRU√á√ïES:

1. Leia atentamente e COMPLETAMENTE todo o texto da not√≠cia, do in√≠cio ao fim. N√£o analise apenas trechos iniciais ou finais - processe o texto inteiro para garantir uma an√°lise precisa.

2. Identifique se a not√≠cia trata de FRAUDES EMPRESARIAIS, incluindo mas n√£o limitado a:
   - Fraude em licita√ß√µes
   - Cartel entre empresas
   - Superfaturamento
   - Corrup√ß√£o envolvendo empresas
   - Lavagem de dinheiro empresarial
   - Forma√ß√£o de organiza√ß√£o criminosa empresarial
   - Contratos fraudulentos
   - Simula√ß√£o de concorr√™ncia
   - Direcionamento de licita√ß√µes
   - Pagamento de propina por empresas
   - Desvio de recursos p√∫blicos envolvendo empresas
   - Falsifica√ß√£o de documentos em processos licitat√≥rios
   - Qualquer outro tipo de fraude que envolva empresas

3. Identifique o n√≠vel de confian√ßa da an√°lise:
   - "alta": A not√≠cia claramente trata de fraude empresarial com detalhes expl√≠citos
   - "m√©dia": A not√≠cia provavelmente trata de fraude empresarial mas com alguns detalhes impl√≠citos
   - "baixa": A not√≠cia menciona fraude de forma tangencial ou n√£o est√° claro

4. Liste os TIPOS DE FRAUDE identificados (exemplos: "fraude em licita√ß√£o", "cartel", "superfaturamento", etc.)

5. Liste as EMPRESAS mencionadas:
   - APENAS extraia nomes de PESSOAS JUR√çDICAS (empresas, raz√µes sociais)
   - Indicadores de empresa: Ltda., S.A., ME, EPP, EIRELI, palavras como "Construtora", "Servi√ßos", "Com√©rcio", "Engenharia", "Loca√ß√µes", etc.
   - N√ÉO inclua nomes de pessoas f√≠sicas aqui
   - Exemplos de EMPRESAS: "Tendas Catarinense Loca√ß√µes Ltda.", "Tri√¢ngulo Engenharia e Consultoria", "Construtora ABC Ltda."
   - Exemplos que N√ÉO s√£o empresas: "Jo√£o Silva", "Maria Santos", "Pedro Oliveira"

6. Liste as PESSOAS ENVOLVIDAS mencionadas COM SEUS PAP√âIS/FUN√á√ïES:
   - APENAS extraia nomes de PESSOAS F√çSICAS (empres√°rios, pol√≠ticos, servidores p√∫blicos, etc.)
   - IMPORTANTE: Inclua o papel/fun√ß√£o da pessoa entre par√™nteses ap√≥s o nome
   - Formato: "Nome Completo (fun√ß√£o/papel)"
   - Indicadores de pessoa: nomes pr√≥prios seguidos de sobrenomes, sem sufixos empresariais
   - Analise o contexto para identificar quem √© a pessoa (empres√°rio, prefeito, servidor, s√≥cio, etc.)
   - N√ÉO inclua raz√µes sociais ou nomes de empresas aqui
   - Exemplos CORRETOS: 
     * "Jo√£o Silva (empres√°rio)"
     * "Pedro Costa (prefeito)"
     * "Maria Santos (s√≥cia da empresa)"
     * "Carlos Oliveira (servidor p√∫blico)"
     * "Ana Lima (ex-prefeita)"
   - Exemplos INCORRETOS: 
     * "Jo√£o Silva" (falta o papel)
     * "Construtora Silva Ltda." (√© empresa, n√£o pessoa)

IMPORTANTE - DIFERENCIA√á√ÉO:
- Se aparecer "Ltda.", "S.A.", "ME", "EPP", "EIRELI" ‚Üí √© EMPRESA
- Se for apenas nome e sobrenome de pessoa ‚Üí √© PESSOA
- Se o texto menciona "o empres√°rio [nome]", "o prefeito [nome]", "o servidor [nome]" ‚Üí √© PESSOA
- Se o texto menciona "a empresa [nome]", "a construtora [nome]" ‚Üí √© EMPRESA
- Em caso de d√∫vida, analise o contexto ao redor do nome no texto

----------------------------------------------------------------------
FORMATO DE RESPOSTA:

Retorne APENAS um JSON v√°lido no formato:

{{
  "is_fraud_related": true ou false,
  "confidence": "alta" ou "m√©dia" ou "baixa",
  "fraud_types": ["tipo1", "tipo2", ...],
  "companies_involved": ["empresa1", "empresa2", ...],
  "people_involved": ["pessoa1", "pessoa2", ...]
}}

IMPORTANTE:
- Se a not√≠cia N√ÉO trata de fraude empresarial, retorne is_fraud_related: false e listas vazias
- Seja preciso e extraia apenas informa√ß√µes expl√≠citas no texto
- N√£o invente ou infira informa√ß√µes que n√£o est√£o no texto

----------------------------------------------------------------------
Texto da not√≠cia:
\"\"\"{full_text}\"\"\"

Responda APENAS com o JSON v√°lido, sem texto adicional.
"""

        start_time = time.time()
        
        # Configurar timeout
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(timeout)
        
        try:
            response = self.llm.invoke([HumanMessage(content=prompt_content)])
            signal.alarm(0)  # Cancelar timeout se completou
            result = response.content.strip()
            parsed_result = self._parse_json_response(result, default_return)
            execution_time = time.time() - start_time
            parsed_result["execution_time_seconds"] = round(execution_time, 2)
            return parsed_result
        except TimeoutError:
            signal.alarm(0)  # Cancelar timeout
            print(f"[‚è±Ô∏è TIMEOUT] Processamento excedeu {timeout}s - pulando not√≠cia")
            execution_time = time.time() - start_time
            default_return["execution_time_seconds"] = round(execution_time, 2)
            return default_return
        except Exception as e:
            signal.alarm(0)  # Cancelar timeout
            error_msg = str(e)
            # Verificar se √© erro 403
            if "403" in error_msg or "Forbidden" in error_msg:
                print(f"[‚ùå ERRO 403] Ollama retornou erro de permiss√£o: {error_msg}")
                raise OllamaError403(f"Erro 403 do Ollama: {error_msg}")
            print(f"[Erro na An√°lise] {e}")
            execution_time = time.time() - start_time
            default_return["execution_time_seconds"] = round(execution_time, 2)
            return default_return

    def _parse_json_response(self, result_str: str, default_return: Dict) -> Dict:
        if result_str.startswith("```json"):
            result_str = result_str[7:]
        if result_str.startswith("```"):
            result_str = result_str[3:]
        if result_str.endswith("```"):
            result_str = result_str[:-3]
        
        result_str = result_str.strip()

        try:
            data = json.loads(result_str)
            
            out = {
                "is_fraud_related": bool(data.get("is_fraud_related", False)),
                "confidence": data.get("confidence", "baixa"),
                "fraud_types": [],
                "companies_involved": [],
                "people_involved": [],
                "execution_time_seconds": 0.0
            }
            
            for key in ["fraud_types", "companies_involved", "people_involved"]:
                value = data.get(key, [])
                if isinstance(value, str):
                    value = [value] if value.strip() else []
                elif not isinstance(value, list):
                    value = []
                
                clean_list = []
                seen = set()
                for item in value:
                    s = str(item).strip().strip('"').strip("'").strip()
                    if s and s not in seen:
                        clean_list.append(s)
                        seen.add(s)
                
                out[key] = clean_list
            
            return out
        except json.JSONDecodeError:
            print(f"Falha ao decodificar JSON. In√≠cio da resposta: {result_str[:50]}...")
            return default_return


def save_csv_incremental(csv_file: str, fraud_news_with_companies: list):
    """
    Salva o CSV incrementalmente com os resultados atuais.
    """
    csv_path = Path(csv_file)
    if fraud_news_with_companies:
        with open(csv_path, 'w', encoding='utf-8', newline='') as f:
            fieldnames = ['file', 'title', 'url', 'text', 'companies', 'people', 'fraud_types', 'confidence', 'execution_time_seconds']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(fraud_news_with_companies)


def get_already_processed_files(output_file: str) -> set:
    """
    Retorna conjunto de arquivos j√° processados do JSON parcial.
    """
    output_path = Path(output_file)
    if not output_path.exists():
        return set()
    
    try:
        with open(output_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        fraud_news = data.get('fraud_news', [])
        return {entry['file'] for entry in fraud_news}
    except:
        return set()


def process_all_news(input_dir: str, output_file: str, csv_file: str, metrics_file: str, resume: bool = True):
    """
    Processa todas as not√≠cias na pasta e identifica aquelas relacionadas a fraudes.
    Gera um CSV com apenas as not√≠cias que t√™m empresas envolvidas em fraudes.
    Gera um arquivo de m√©tricas de performance separado.
    Salva incrementalmente a cada 25 not√≠cias processadas.
    Inclui timeout de 60s por not√≠cia para evitar travamentos.
    """
    detector = FraudDetector()
    input_path = Path(input_dir)
    
    if not input_path.exists():
        print(f"ERRO: Diret√≥rio {input_dir} n√£o encontrado!")
        return
    
    json_files = sorted(list(input_path.glob("*.json")))
    total_files = len(json_files)
    
    # Verificar arquivos j√° processados
    already_processed = get_already_processed_files(output_file) if resume else set()
    
    # Carregar dados parciais se existirem
    fraud_news = []
    fraud_news_with_companies = []
    if resume and already_processed:
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            fraud_news = data.get('fraud_news', [])
            # Reconstruir lista de CSV
            for entry in fraud_news:
                analysis = entry.get('analysis', {})
                if analysis.get('companies_involved'):
                    fraud_news_with_companies.append({
                        'file': entry['file'],
                        'title': entry['title'],
                        'url': entry['url'],
                        'text': '',  # Ser√° preenchido depois se necess√°rio
                        'companies': '; '.join(analysis['companies_involved']),
                        'people': '; '.join(analysis.get('people_involved', [])),
                        'fraud_types': '; '.join(analysis.get('fraud_types', [])),
                        'confidence': analysis.get('confidence', ''),
                        'execution_time_seconds': analysis.get('execution_time_seconds', 0)
                    })
            print(f"\nüìÇ RETOMANDO PROCESSAMENTO")
            print(f"   J√° processadas: {len(already_processed)} not√≠cias")
            print(f"   Fraudes detectadas anteriormente: {len(fraud_news)}")
            print(f"   Com empresas: {len(fraud_news_with_companies)}")
        except:
            already_processed = set()
    
    print(f"\n{'='*70}")
    print(f"Iniciando processamento de {total_files} not√≠cias...")
    print(f"üíæ Salvamento autom√°tico a cada 25 not√≠cias")
    print(f"‚è±Ô∏è  Timeout: {TIMEOUT_SECONDS}s por not√≠cia")
    if START_FROM > 0:
        print(f"‚û°Ô∏è  Come√ßando da not√≠cia {START_FROM} (pulando 1-{START_FROM-1})")
    if already_processed:
        print(f"üîÑ Modo retomada: pulando {len(already_processed)} j√° processadas")
    print(f"{'='*70}\n")
    
    processed = len(already_processed)
    skipped = 0
    timeouts = 0
    consecutive_403_errors = 0  # Contador de erros 403 consecutivos
    SAVE_INTERVAL = 25
    
    for index, json_file in enumerate(json_files, start=1):
        # Usar √≠ndice na lista ordenada como "n√∫mero da not√≠cia"
        news_number = index
        
        # Pular se antes do START_FROM
        if START_FROM > 0 and news_number < START_FROM:
            skipped += 1
            continue
        
        # Pular se j√° processado
        if json_file.name in already_processed:
            skipped += 1
            continue
        
        processed += 1
        
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                news_data = json.load(f)
            
            title = news_data.get("title", "")
            text = news_data.get("text", "")
            url = news_data.get("url", "")
            
            print(f"[{news_number}/{total_files}] Processando: {json_file.name}...")
            
            try:
                result = detector.analyze_fraud(text, title, timeout=TIMEOUT_SECONDS)
                # Resetar contador de 403 em caso de sucesso
                consecutive_403_errors = 0
            except OllamaError403 as e403:
                consecutive_403_errors += 1
                print(f"‚ö†Ô∏è  Erro 403 consecutivo #{consecutive_403_errors}/{MAX_CONSECUTIVE_403_ERRORS}")
                
                if consecutive_403_errors >= MAX_CONSECUTIVE_403_ERRORS:
                    print(f"\n{'='*70}")
                    print(f"üõë INTERROMPENDO PROCESSAMENTO")
                    print(f"   Motivo: {MAX_CONSECUTIVE_403_ERRORS} erros 403 consecutivos do Ollama")
                    print(f"   √öltima not√≠cia processada: {json_file.name}")
                    print(f"   Total processadas at√© agora: {processed}")
                    print(f"{'='*70}\n")
                    
                    # Salvar progresso antes de parar
                    output_path = Path(output_file)
                    with open(output_path, 'w', encoding='utf-8') as f:
                        json.dump({
                            "total_processed": processed,
                            "total_fraud_related": len(fraud_news),
                            "total_with_companies": len(fraud_news_with_companies),
                            "stopped_reason": f"M√∫ltiplos erros 403 consecutivos ({MAX_CONSECUTIVE_403_ERRORS})",
                            "last_file": json_file.name,
                            "fraud_news": fraud_news
                        }, f, ensure_ascii=False, indent=2)
                    save_csv_incremental(csv_file, fraud_news_with_companies)
                    print("üíæ Progresso salvo antes de parar.")
                    return  # Parar processamento
                
                # Continuar para pr√≥xima not√≠cia ap√≥s erro 403
                continue
            
            # Contar timeouts
            if result.get('execution_time_seconds', 0) >= TIMEOUT_SECONDS - 1:
                timeouts += 1
            
            if result["is_fraud_related"]:
                fraud_entry = {
                    "file": json_file.name,
                    "title": title,
                    "url": url,
                    "analysis": result
                }
                fraud_news.append(fraud_entry)
                
                print(f"  ‚úì FRAUDE DETECTADA (confian√ßa: {result['confidence']}) - Tempo: {result.get('execution_time_seconds', 0)}s")
                print(f"    Tipos: {', '.join(result['fraud_types'])}")
                if result['companies_involved']:
                    print(f"    Empresas: {', '.join(result['companies_involved'])}")
                if result['people_involved']:
                    print(f"    Pessoas: {', '.join(result['people_involved'])}")
                
                if result['companies_involved']:
                    fraud_news_with_companies.append({
                        "file": json_file.name,
                        "title": title,
                        "url": url,
                        "text": text,
                        "companies": '; '.join(result['companies_involved']),
                        "people": '; '.join(result['people_involved']) if result['people_involved'] else '',
                        "fraud_types": '; '.join(result['fraud_types']),
                        "confidence": result['confidence'],
                        "execution_time_seconds": result.get('execution_time_seconds', 0)
                    })
                    if result['people_involved']:
                        print(f"    üí∞ B√îNUS: Pessoas tamb√©m identificadas!")
                else:
                    if result['people_involved']:
                        print(f"    ‚ö† Apenas pessoas identificadas (sem empresas) - n√£o ser√° inclu√≠da no CSV")
                    else:
                        print(f"    ‚ö† Sem empresas identificadas - n√£o ser√° inclu√≠da no CSV")
            else:
                print(f"  ‚úó N√£o relacionada a fraude empresarial")
        
        except Exception as e:
            print(f"  ‚úó ERRO ao processar {json_file.name}: {e}")
            continue
        
        # Salvamento incremental a cada 25 not√≠cias
        if processed % SAVE_INTERVAL == 0:
            print(f"\n{'='*70}")
            print(f"üíæ SALVAMENTO AUTOM√ÅTICO - {processed}/{total_files} not√≠cias processadas")
            print(f"{'='*70}")
            
            # Salvar JSON parcial
            output_path = Path(output_file)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump({
                    "total_processed": processed,
                    "total_fraud_related": len(fraud_news),
                    "total_with_companies": len(fraud_news_with_companies),
                    "fraud_news": fraud_news
                }, f, ensure_ascii=False, indent=2)
            
            # Salvar CSV parcial
            save_csv_incremental(csv_file, fraud_news_with_companies)
            
            print(f"‚úì JSON salvo: {len(fraud_news)} fraudes detectadas")
            print(f"‚úì CSV salvo: {len(fraud_news_with_companies)} not√≠cias com empresas")
            print(f"{'='*70}\n")
        
        print()
    
    all_execution_times = [entry['analysis'].get('execution_time_seconds', 0) for entry in fraud_news]
    total_execution_time = sum(all_execution_times)
    avg_execution_time = total_execution_time / len(all_execution_times) if all_execution_times else 0
    min_execution_time = min(all_execution_times) if all_execution_times else 0
    max_execution_time = max(all_execution_times) if all_execution_times else 0
    
    sorted_times = sorted(all_execution_times)
    median_execution_time = sorted_times[len(sorted_times)//2] if sorted_times else 0
    
    print(f"\n{'='*70}")
    print(f"PROCESSAMENTO CONCLU√çDO")
    print(f"{'='*70}")
    print(f"Total de not√≠cias processadas: {processed}")
    if skipped > 0:
        print(f"Not√≠cias puladas (j√° processadas): {skipped}")
    if timeouts > 0:
        print(f"‚è±Ô∏è  Not√≠cias com timeout: {timeouts}")
    print(f"Not√≠cias relacionadas a fraudes: {len(fraud_news)}")
    print(f"Not√≠cias com empresas/pessoas identificadas: {len(fraud_news_with_companies)}")
    print(f"\nM√©tricas de Performance:")
    print(f"  Tempo total: {total_execution_time:.2f}s ({total_execution_time/60:.2f} min)")
    print(f"  Tempo m√©dio por not√≠cia: {avg_execution_time:.2f}s")
    print(f"  Tempo m√≠nimo: {min_execution_time:.2f}s")
    print(f"  Tempo m√°ximo: {max_execution_time:.2f}s")
    print(f"  Tempo mediano: {median_execution_time:.2f}s")
    print(f"{'='*70}\n")
    
    output_path = Path(output_file)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump({
            "total_processed": processed,
            "total_fraud_related": len(fraud_news),
            "total_with_companies": len(fraud_news_with_companies),
            "fraud_news": fraud_news
        }, f, ensure_ascii=False, indent=2)
    
    print(f"Resultados JSON salvos em: {output_file}")
    
    metrics_data = {
        "model": SELECTED_MODEL,
        "ollama_host": OLLAMA_HOST,
        "temperature": LLM_TEMPERATURE,
        "timestamp": datetime.now().isoformat(),
        "processing_summary": {
            "total_news_processed": processed,
            "total_fraud_detected": len(fraud_news),
            "total_with_companies_or_people": len(fraud_news_with_companies),
            "fraud_detection_rate": round(len(fraud_news) / processed * 100, 2) if processed > 0 else 0,
            "companies_people_identification_rate": round(len(fraud_news_with_companies) / len(fraud_news) * 100, 2) if fraud_news else 0
        },
        "execution_metrics": {
            "total_time_seconds": round(total_execution_time, 2),
            "total_time_minutes": round(total_execution_time / 60, 2),
            "average_time_per_news": round(avg_execution_time, 2),
            "min_time_seconds": round(min_execution_time, 2),
            "max_time_seconds": round(max_execution_time, 2),
            "median_time_seconds": round(median_execution_time, 2)
        },
        "confidence_distribution": {
            "alta": sum(1 for entry in fraud_news if entry['analysis'].get('confidence') == 'alta'),
            "m√©dia": sum(1 for entry in fraud_news if entry['analysis'].get('confidence') == 'm√©dia'),
            "baixa": sum(1 for entry in fraud_news if entry['analysis'].get('confidence') == 'baixa')
        }
    }
    
    metrics_path = Path(metrics_file)
    with open(metrics_path, 'w', encoding='utf-8') as f:
        json.dump(metrics_data, f, ensure_ascii=False, indent=2)
    
    print(f"M√©tricas de performance salvas em: {metrics_file}")
    
    csv_path = Path(csv_file)
    with open(csv_path, 'w', encoding='utf-8', newline='') as f:
        if fraud_news_with_companies:
            fieldnames = ['file', 'title', 'url', 'companies', 'people', 'fraud_types', 'confidence', 'summary', 'execution_time_seconds']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(fraud_news_with_companies)
            print(f"CSV com not√≠cias de fraude empresarial salvo em: {csv_file}")
        else:
            print(f"‚ö† Nenhuma not√≠cia com empresas ou pessoas identificadas - CSV n√£o criado")
    
    print(f"\n{'='*70}")
    print("RESUMO DAS FRAUDES COM EMPRESAS/PESSOAS IDENTIFICADAS:")
    print(f"{'='*70}\n")
    
    total_time = sum(entry.get('execution_time_seconds', 0) for entry in fraud_news_with_companies)
    avg_time = total_time / len(fraud_news_with_companies) if fraud_news_with_companies else 0
    
    print(f"Tempo total de processamento: {total_time:.2f}s")
    print(f"Tempo m√©dio por not√≠cia: {avg_time:.2f}s")
    print(f"{'='*70}\n")
    
    for i, entry in enumerate(fraud_news_with_companies, 1):
        print(f"{i}. {entry['file']} (Tempo: {entry.get('execution_time_seconds', 0)}s)")
        print(f"   T√≠tulo: {entry['title'][:80]}...")
        if entry.get('companies'):
            print(f"   Empresas: {entry['companies']}")
        if entry.get('people'):
            print(f"   Pessoas: {entry['people']}")
        print(f"   Tipos de Fraude: {entry['fraud_types']}")
        print(f"   Confian√ßa: {entry['confidence']}")
        print()


if __name__ == "__main__":
    INPUT_DIR = "/home/paulo/projects/main-server/.PAULO/dataset_building/ndmais_articles_json"
    OUTPUT_JSON = "/home/paulo/projects/main-server/.PAULO/fraud_detection_ndmais_results.json"
    OUTPUT_CSV = "/home/paulo/projects/main-server/.PAULO/fraud_news_ndmais_with_companies.csv"
    OUTPUT_METRICS = "/home/paulo/projects/main-server/.PAULO/performance_metrics_ndmais.json"
    
    print("\n" + "="*70)
    print("DETECTOR DE FRAUDES EMPRESARIAIS EM NOT√çCIAS")
    print("="*70)
    print(f"Diret√≥rio de entrada: {INPUT_DIR}")
    print(f"Arquivo JSON de sa√≠da: {OUTPUT_JSON}")
    print(f"Arquivo CSV de sa√≠da: {OUTPUT_CSV}")
    print(f"Arquivo de m√©tricas: {OUTPUT_METRICS}")
    print(f"Modelo: {SELECTED_MODEL}")
    print("="*70 + "\n")
    
    process_all_news(INPUT_DIR, OUTPUT_JSON, OUTPUT_CSV, OUTPUT_METRICS)
